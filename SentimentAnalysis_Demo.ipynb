{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2f5ab8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Reviews\n",
    "\n",
    "This notebook demonstrates sentiment analysis on IMDB movie reviews using both classic and TF-IDF/n-gram features, multiple classifiers, and detailed evaluation and visualization. The workflow includes data download, preprocessing, feature engineering, model training, evaluation, and result visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a460136",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import all necessary libraries for data processing, feature extraction, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import html\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29049ae1",
   "metadata": {},
   "source": [
    "## 2. Download and Extract IMDB Data\n",
    "Download the IMDB dataset and extract it for use in sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    url = 'https://www.dropbox.com/s/8oehplrobcgi9cq/imdb.tgz?dl=1'\n",
    "    if not os.path.exists('imdb.tgz'):\n",
    "        print('Downloading dataset...')\n",
    "        urllib.request.urlretrieve(url, 'imdb.tgz')\n",
    "    else:\n",
    "        print('imdb.tgz already exists.')\n",
    "    if not os.path.exists('data'):\n",
    "        print('Extracting dataset...')\n",
    "        tar = tarfile.open('imdb.tgz')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print('Data directory already exists.')\n",
    "\n",
    "# Download and extract data if needed\n",
    "\n",
    "# Uncomment the line below to run in notebook\n",
    "# download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888df202",
   "metadata": {},
   "source": [
    "## 3. Read and Preprocess Data\n",
    "Read the training and test data, clean the text, and prepare the labels for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a34892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def read_data(path):\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'pos', '*.txt'))])\n",
    "    data = [(1, clean_text(open(f).readlines()[0])) for f in sorted(fnames)]\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'neg', '*.txt'))])\n",
    "    data += [(0, clean_text(open(f).readlines()[0])) for f in sorted(fnames)]\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    return np.array([d[1] for d in data]), np.array([d[0] for d in data])\n",
    "\n",
    "# Uncomment to download and extract data\n",
    "# download_data()\n",
    "\n",
    "# Load training and test data\n",
    "docs_train, labels_train = read_data(os.path.join('data', 'train'))\n",
    "docs_test, labels_test = read_data(os.path.join('data', 'test'))\n",
    "print(f\"Loaded {len(docs_train)} training and {len(docs_test)} test documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f983e29",
   "metadata": {},
   "source": [
    "## 4. Tokenization and Feature Extraction Functions\n",
    "Define and demonstrate the tokenization and feature extraction functions used for classic feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, keep_internal_punct=False):\n",
    "    doc = doc.lower()\n",
    "    if not keep_internal_punct:\n",
    "        return np.array(re.sub(r'\\W+', ' ', doc).split())\n",
    "    else:\n",
    "        return np.array([re.sub(r'^\\W+|\\W+$', '', t) for t in doc.split()])\n",
    "\n",
    "def token_features(tokens, feats):\n",
    "    for t in tokens:\n",
    "        feats[f\"token={t}\"] += 1\n",
    "\n",
    "def token_pair_features(tokens, feats, k=3):\n",
    "    for i in range(len(tokens) - k + 1):\n",
    "        window = tokens[i:i+k]\n",
    "        for a, b in combinations(window, 2):\n",
    "            feats[f\"token_pair={a}__{b}\"] += 1\n",
    "\n",
    "neg_words = set(['bad', 'hate', 'horrible', 'worst', 'boring'])\n",
    "pos_words = set(['awesome', 'amazing', 'best', 'good', 'great', 'love', 'wonderful'])\n",
    "def lexicon_features(tokens, feats):\n",
    "    feats['neg_words'] = 0\n",
    "    feats['pos_words'] = 0\n",
    "    for t in tokens:\n",
    "        if t.lower() in neg_words:\n",
    "            feats['neg_words'] += 1\n",
    "        if t.lower() in pos_words:\n",
    "            feats['pos_words'] += 1\n",
    "\n",
    "# Demonstrate on a sample document\n",
    "sample_doc = \"I LOVE this great movie, but the ending was bad.\"\n",
    "tokens = tokenize(sample_doc)\n",
    "feats = defaultdict(int)\n",
    "token_features(tokens, feats)\n",
    "token_pair_features(tokens, feats)\n",
    "lexicon_features(tokens, feats)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Features:\", dict(feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e1b8b",
   "metadata": {},
   "source": [
    "## 5. Vectorization of Documents\n",
    "Convert tokenized documents into sparse feature matrices using the defined feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(tokens, feature_fns):\n",
    "    feats = defaultdict(int)\n",
    "    for fn in feature_fns:\n",
    "        fn(tokens, feats)\n",
    "    return sorted(feats.items())\n",
    "\n",
    "def vectorize(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    feats_count = defaultdict(int)\n",
    "    feats_per_doc = []\n",
    "    for tokens in tokens_list:\n",
    "        feats = featurize(tokens, feature_fns)\n",
    "        feats_per_doc.append(feats)\n",
    "        for f, c in feats:\n",
    "            feats_count[f] += 1\n",
    "    if vocab is None:\n",
    "        vocab = {f: i for i, f in enumerate(sorted([f for f in feats_count if feats_count[f] >= min_freq]))}\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, feats in enumerate(feats_per_doc):\n",
    "        for f, c in feats:\n",
    "            if f in vocab:\n",
    "                rows.append(i)\n",
    "                cols.append(vocab[f])\n",
    "                data.append(c)\n",
    "    X = csr_matrix((data, (rows, cols)), shape=(len(tokens_list), len(vocab)), dtype=np.int64)\n",
    "    return X, vocab\n",
    "\n",
    "# Example: Vectorize first 3 training docs\n",
    "feature_fns = [token_features, token_pair_features, lexicon_features]\n",
    "tokens_list = [tokenize(d) for d in docs_train[:3]]\n",
    "X_sample, vocab_sample = vectorize(tokens_list, feature_fns, min_freq=1)\n",
    "print(\"Feature matrix shape:\", X_sample.shape)\n",
    "print(\"Vocabulary:\", vocab_sample)\n",
    "print(\"Feature matrix (dense):\\n\", X_sample.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac87ff",
   "metadata": {},
   "source": [
    "## 6. Evaluate Feature Combinations with Cross-Validation\n",
    "Test different feature settings and compute cross-validation accuracy for each combination using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14771daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(truth, predicted):\n",
    "    return np.mean(truth == predicted)\n",
    "\n",
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    kf = KFold(n_splits=k, shuffle=False, random_state=42)\n",
    "    accs = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        clf.fit(X[train_idx], labels[train_idx])\n",
    "        preds = clf.predict(X[test_idx])\n",
    "        accs.append(accuracy_score(labels[test_idx], preds))\n",
    "    return np.mean(accs)\n",
    "\n",
    "def eval_all_combinations(docs, labels, punct_vals, feature_fns, min_freqs):\n",
    "    results = []\n",
    "    all_feature_combos = []\n",
    "    for i in range(len(feature_fns)):\n",
    "        for combo in combinations(feature_fns, i+1):\n",
    "            all_feature_combos.append(list(combo))\n",
    "    for punct in punct_vals:\n",
    "        tokens_list = [tokenize(d, punct) for d in docs]\n",
    "        for min_freq in min_freqs:\n",
    "            for feats in all_feature_combos:\n",
    "                X, vocab = vectorize(tokens_list, feats, min_freq)\n",
    "                acc = cross_validation_accuracy(LogisticRegression(), X, labels, 5)\n",
    "                results.append({'punct': punct, 'features': feats, 'min_freq': min_freq, 'accuracy': acc})\n",
    "    return sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "# Evaluate all combinations (may take a while on full data)\n",
    "# results = eval_all_combinations(docs_train, labels_train, [True, False], feature_fns, [2, 5, 10])\n",
    "# print('Best result:', results[0])\n",
    "# print('Worst result:', results[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4237ff",
   "metadata": {},
   "source": [
    "## 7. Plot Sorted Cross-Validation Accuracies\n",
    "Visualize the cross-validation accuracies for all feature settings using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bcc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted_accuracies(results):\n",
    "    accuracies = sorted([r['accuracy'] for r in results])\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(accuracies)\n",
    "    plt.xlabel('Setting')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Sorted Cross-Validation Accuracies')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('accuracies.png')\n",
    "    plt.show()\n",
    "    print('Plot saved as accuracies.png')\n",
    "\n",
    "# Example usage (uncomment after running eval_all_combinations):\n",
    "# plot_sorted_accuracies(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb832852",
   "metadata": {},
   "source": [
    "## 8. Analyze Mean Accuracy per Setting\n",
    "Compute and display the mean accuracy for each model setting to understand their impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71301583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_accuracy_per_setting(results):\n",
    "    setting_accs = defaultdict(list)\n",
    "    for r in results:\n",
    "        # Features\n",
    "        feats = 'features=' + ' '.join(fn.__name__ for fn in r['features'])\n",
    "        setting_accs[feats].append(r['accuracy'])\n",
    "        # Punctuation\n",
    "        setting_accs[f'punct={r[\"punct\"]}'].append(r['accuracy'])\n",
    "        # Min freq\n",
    "        setting_accs[f'min_freq={r[\"min_freq\"]}'].append(r['accuracy'])\n",
    "    mean_acc = [(np.mean(v), k) for k, v in setting_accs.items()]\n",
    "    return sorted(mean_acc, reverse=True)\n",
    "\n",
    "# Example usage (uncomment after running eval_all_combinations):\n",
    "# for acc, setting in mean_accuracy_per_setting(results):\n",
    "#     print(f'{setting}: {acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261549f",
   "metadata": {},
   "source": [
    "## 9. Train Best Classifier and Show Top Coefficients\n",
    "Train a Logistic Regression classifier on the best feature settings and display the top positive and negative coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e75a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_best_classifier(docs, labels, best_result):\n",
    "    punct = best_result['punct']\n",
    "    feature_fns = best_result['features']\n",
    "    min_freq = best_result['min_freq']\n",
    "    tokens_list = [tokenize(d, punct) for d in docs]\n",
    "    X, vocab = vectorize(tokens_list, feature_fns, min_freq)\n",
    "    clf = LogisticRegression().fit(X, labels)\n",
    "    return clf, vocab\n",
    "\n",
    "def top_coefs(clf, label, n, vocab):\n",
    "    coef = clf.coef_[0]\n",
    "    idx_to_feat = {v: k for k, v in vocab.items()}\n",
    "    if label == 1:\n",
    "        topn = np.argsort(coef)[-n:][::-1]\n",
    "    else:\n",
    "        topn = np.argsort(coef)[:n]\n",
    "    return [(idx_to_feat[i], coef[i]) for i in topn]\n",
    "\n",
    "# Example usage (uncomment after running eval_all_combinations):\n",
    "# clf, vocab = fit_best_classifier(docs_train, labels_train, results[0])\n",
    "# print('Top negative coefficients:')\n",
    "# print(top_coefs(clf, 0, 5, vocab))\n",
    "# print('Top positive coefficients:')\n",
    "# print(top_coefs(clf, 1, 5, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57525b53",
   "metadata": {},
   "source": [
    "## 10. Test Set Evaluation and Misclassification Analysis\n",
    "Evaluate the trained classifier on the test set, show accuracy, classification report, confusion matrix, and print the top misclassified documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_data(best_result, vocab):\n",
    "    punct = best_result['punct']\n",
    "    feature_fns = best_result['features']\n",
    "    min_freq = best_result['min_freq']\n",
    "    test_docs, test_labels = read_data(os.path.join('data', 'test'))\n",
    "    tokens_list = [tokenize(d, punct) for d in test_docs]\n",
    "    X_test, _ = vectorize(tokens_list, feature_fns, min_freq, vocab)\n",
    "    return test_docs, test_labels, X_test\n",
    "\n",
    "def print_top_misclassified(test_docs, test_labels, X_test, clf, n):\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    preds = clf.predict(X_test)\n",
    "    misclassified = []\n",
    "    for i in range(len(test_labels)):\n",
    "        if preds[i] != test_labels[i]:\n",
    "            prob = probs[i][preds[i]]\n",
    "            misclassified.append((test_labels[i], preds[i], prob, test_docs[i]))\n",
    "    misclassified = sorted(misclassified, key=lambda x: -x[2])\n",
    "    for i in misclassified[:n]:\n",
    "        print(f\"\\nTruth={i[0]} Predicted={i[1]} Proba={i[2]:.6f}\\n{i[3]}\")\n",
    "\n",
    "# Example usage (uncomment after fitting best classifier):\n",
    "# test_docs, test_labels, X_test = parse_test_data(results[0], vocab)\n",
    "# preds = clf.predict(X_test)\n",
    "# print('Test accuracy:', accuracy_score(test_labels, preds))\n",
    "# print(classification_report(test_labels, preds, digits=4))\n",
    "# print('Confusion matrix:')\n",
    "# print(confusion_matrix(test_labels, preds))\n",
    "# print_top_misclassified(test_docs, test_labels, X_test, clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8dfbb2",
   "metadata": {},
   "source": [
    "## 11. TF-IDF and N-gram Feature Extraction with Multiple Models\n",
    "Demonstrate feature extraction using TfidfVectorizer with different n-gram ranges and train multiple classifiers (Logistic Regression, SVM, Random Forest, Naive Bayes) with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77898e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(name):\n",
    "    if name == 'logreg':\n",
    "        return LogisticRegression(max_iter=200)\n",
    "    elif name == 'svm':\n",
    "        return LinearSVC(max_iter=2000)\n",
    "    elif name == 'rf':\n",
    "        return RandomForestClassifier(n_estimators=100)\n",
    "    elif name == 'nb':\n",
    "        return MultinomialNB()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier: {name}\")\n",
    "\n",
    "# Example: TF-IDF with unigrams and bigrams, multiple models\n",
    "# ngram_range = (1,2)\n",
    "# vectorizer = TfidfVectorizer(lowercase=True, ngram_range=ngram_range, stop_words='english')\n",
    "# X = vectorizer.fit_transform(docs_train)\n",
    "# models = ['logreg', 'svm', 'rf', 'nb']\n",
    "# for model in models:\n",
    "#     clf = get_classifier(model)\n",
    "#     kf = KFold(n_splits=5, shuffle=False, random_state=42)\n",
    "#     accs = []\n",
    "#     for train_idx, test_idx in kf.split(X):\n",
    "#         clf.fit(X[train_idx], labels_train[train_idx])\n",
    "#         preds = clf.predict(X[test_idx])\n",
    "#         accs.append(accuracy_score(labels_train[test_idx], preds))\n",
    "#     print(f\"{model} mean CV accuracy: {np.mean(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bec67",
   "metadata": {},
   "source": [
    "## 12. Feature Importance and ROC Curve Visualization\n",
    "Plot feature importances for Logistic Regression and Random Forest models, and plot/save the ROC curve for classifiers that support probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_names, importances, top_n=10, title='Feature Importance', filename='feature_importance.png'):\n",
    "    importances = np.array(importances)\n",
    "    indices = np.argsort(np.abs(importances))[-top_n:][::-1]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(top_n), importances[indices], align='center')\n",
    "    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(f'Feature importance plot saved as {filename}')\n",
    "\n",
    "def plot_roc_curve(y_true, y_score, filename='roc_curve.png'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(f'ROC curve saved as {filename}')\n",
    "\n",
    "# Example usage (after fitting model):\n",
    "# if hasattr(clf, 'coef_'):\n",
    "#     plot_feature_importance(feature_names, clf.coef_[0])\n",
    "# if hasattr(clf, 'feature_importances_'):\n",
    "#     plot_feature_importance(feature_names, clf.feature_importances_)\n",
    "# if hasattr(clf, 'predict_proba'):\n",
    "#     probs = clf.predict_proba(X_test)[:, 1]\n",
    "#     plot_roc_curve(labels_test, probs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
